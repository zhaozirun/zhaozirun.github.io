<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>
<meta name="description" content="FedProx 作者：Tian Li，Anit Kumar Sahu，Manzil Zaheer，Maziar Sanjabi，Ameet Talwalkar，Virginia Smithan Li1Anit Kur   摘要 联邦学习的两大挑战：网络每个设备的系统特征存在显著差异（系统异构性），非独立同分布数据（统计不确定性，统计数据异构性）  框架FedProx，来解决联邦网络的异构性。  F">
<meta property="og:type" content="article">
<meta property="og:title" content="FedProx">
<meta property="og:url" content="http://example.com/2021/11/02/FedProx/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="FedProx 作者：Tian Li，Anit Kumar Sahu，Manzil Zaheer，Maziar Sanjabi，Ameet Talwalkar，Virginia Smithan Li1Anit Kur   摘要 联邦学习的两大挑战：网络每个设备的系统特征存在显著差异（系统异构性），非独立同分布数据（统计不确定性，统计数据异构性）  框架FedProx，来解决联邦网络的异构性。  F">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2021/png/22220737/1635391276449-66528b39-97ec-4d3c-a692-a686ae6c93a7.png#clientId=u67589db9-23d9-4&from=paste&height=400&id=u02b42f55&margin=%5Bobject%20Object%5D&name=image.png&originHeight=800&originWidth=1228&originalType=binary&ratio=1&size=175124&status=done&style=none&taskId=u77b545d7-928c-4db5-be4d-65bddcade26&width=614">
<meta property="article:published_time" content="2021-11-02T13:26:25.000Z">
<meta property="article:modified_time" content="2021-11-02T14:13:40.998Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2021/png/22220737/1635391276449-66528b39-97ec-4d3c-a692-a686ae6c93a7.png#clientId=u67589db9-23d9-4&from=paste&height=400&id=u02b42f55&margin=%5Bobject%20Object%5D&name=image.png&originHeight=800&originWidth=1228&originalType=binary&ratio=1&size=175124&status=done&style=none&taskId=u77b545d7-928c-4db5-be4d-65bddcade26&width=614">


<link rel="canonical" href="http://example.com/2021/11/02/FedProx/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/11/02/FedProx/","path":"2021/11/02/FedProx/","title":"FedProx"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>FedProx | Hexo</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#FedProx"><span class="nav-number">1.</span> <span class="nav-text">FedProx</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.2.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%83%8C%E6%99%AF%E5%92%8C%E7%9B%B8%E5%85%B3%E6%96%87%E7%8C%AE"><span class="nav-number">1.3.</span> <span class="nav-text">背景和相关文献</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%A1%88"><span class="nav-number">1.4.</span> <span class="nav-text">方案</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FedProx%E7%9A%84%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90"><span class="nav-number">1.5.</span> <span class="nav-text">FedProx的收敛分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.6.</span> <span class="nav-text">实验</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/02/FedProx/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          FedProx
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2021-11-02 21:26:25 / Modified: 22:13:40" itemprop="dateCreated datePublished" datetime="2021-11-02T21:26:25+08:00">2021-11-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="FedProx"><a href="#FedProx" class="headerlink" title="FedProx"></a>FedProx</h1><ul>
<li><p>作者：Tian Li，Anit Kumar Sahu，Manzil Zaheer，Maziar Sanjabi，Ameet Talwalkar，Virginia Smithan Li1Anit Kur  </p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul>
<li><p>联邦学习的两大挑战：网络每个设备的系统特征存在显著差异（系统异构性），非独立同分布数据（统计不确定性，统计数据异构性）</p>
</li>
<li><p>框架FedProx，来解决联邦网络的异构性。</p>
</li>
<li><p>FedProx可以看作是FedAvg的泛化和重新参数化。</p>
</li>
<li><p>尽管重新参数化只对方法本身进行了微小的修改，但这些修改在理论和实践上都具有重要影响。</p>
</li>
<li><p>从理论上讲，当从非独立同分布（统计异质性）学习数据时，我们的框架提供了收敛保证，同时通过允许每个参与设备执行可变工作量（系统异质性）来遵守设备级系统约束。</p>
</li>
<li><p>实际上，我们通过一组真实的联邦数据集证明了FEDProx比FedAvg具有更健壮的收敛性。特别是，相对于FedAvg来说，在高度异质性的环境中，FedProx显示出明显更稳定和准确的收敛行为，平均将绝对测试精度提高了22%。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2></li>
<li><p>FedAvg</p>
<ul>
<li>E local epoches, K devices each global epoch</li>
<li>未完全解决下述异构性挑战：<ul>
<li>系统异构性方面：FedAvg基于底层系统约束，不允许参与设备执行可变数量的本地工作；取而代之的是，FedAvg会简单丢弃在具体的时间窗口下不能完成E轮计算的设备。 </li>
<li>从统计数据方面：FedAvg在设备间为非独立同分布数据时会出现经验分歧（diverge empirically）。</li>
</ul>
</li>
<li>对于现实场景很难进行理论分析，因此缺少收敛保证来描述其特征。（Section 2更多细节）</li>
</ul>
</li>
<li><p>提出FedProx</p>
<ul>
<li>一个联邦优化算法：解决（理论和经验的——对标异构的系统和数据）异构挑战</li>
<li>提出FedProx的一个关键洞察点在于：联邦学习中系统和统计异构性的相互作用</li>
<li>事实上，丢弃落伍设备和天真的合并落伍设备的部分信息，会隐式地增加统计异质性，并可能对收敛行为产生不利影响。（*）</li>
<li>为了缓解（*）问题，计划在对目标函数加入一个近侧项（proximal term）来帮助提升方法的稳定性。该项为服务器提供了一个有理可循的方法来说明和部分信息相关的异质性。（描述异质性）</li>
<li>理论上来说，这些修改允许我们为我们的方法提供收敛保证，和分析异构性的影响。</li>
<li>实际上，我们证明了修改提升了稳定性和整体的联邦学习在异构网络中的精确度——提升了在高度异构设置下约22%的绝对测试精度。</li>
</ul>
</li>
<li><p>文章结构</p>
<ul>
<li>本文的其余部分组织如下。在第2节中，我们提供了联邦学习的背景和相关工作的概述。然后，我们在第3节介绍了我们提出的框架FedProx，并在第4节推导了该框架的收敛保证，该框架考虑了统计数据和系统的异质性。最后，在第5节中，我们提供了一套完整的合成和真实世界联邦数据集的实验评估。我们的实证结果有助于说明和验证我们的理论分析，并证明了在异构网络中FedProx对FedAvg的实际改进。<h2 id="背景和相关文献"><a href="#背景和相关文献" class="headerlink" title="背景和相关文献"></a>背景和相关文献</h2></li>
</ul>
</li>
<li><p>最近提出的优化方法一般对应于具体的联邦学习挑战。这些方法对传统对分布式方法例如ADMM有重大提升，或对mini-batch方法有重大提升。对mini-batch提升的途径主要包括： 允许不精确的本地更新，以平衡大型网络的通信和计算；允许任意通信轮的激活一组设备子集。</p>
</li>
<li><p>Smith 2017:提供通信有效的原始对偶优化方法，该方法通过多任务学习框架——可以对每个设备学习分立但关联的模型。</p>
</li>
<li><p>尽管具备理论保证和实验有效性，该方法对非凸问题并不通用，例如机器学习，其中强对偶性不再得到保证。</p>
</li>
<li><p>在非凸设置中，FedAvg，一个基于平均本地随机梯度下降更新的启发式的方法,实际工作效果很好。</p>
</li>
<li><p>不幸的是，由于FedAvg的本地更新方案，FedAvgis的分析非常具有挑战性。因为几乎没有设备在每一轮都处于活动状态，以及数据在网络中通常以异构状态分布的问题。特别是，由于每个设备都生成自己的本地数据，因此统计不确定性与设备之间不相同分布的数据是常见的。有几项工作通过一些步骤来针对更简单、非联邦集中的FedAvg的分析。例如，并行SGD和相关变体，其执行了与FedAvg类似的本地更新，但是在IID环境中的。然而，结果依赖于一个前提：每个局部solver都是同一随机过程的副本（由于IID假设）。因此这条推理路线不适用于异构环境。</p>
</li>
<li><p>尽管相关工作探索了统计异构设置下的收敛保证，但他们设立了限制性的假设，即所有的设备都参与了每一轮的通信，这在真实的联邦网络中是不可能的。此外，与本文提出的解算器不可知框架相比，它们依赖于每个设备（SGD或GD）使用的特定解算器，并在分析中添加了额外的凸性假设（Wang et al.，2019）或统一边界梯度假设（Yu et al.，2018）。还有一些启发式方法旨在通过共享本地设备数据或服务器端代理数据来解决统计异质性问题。然而，这些方法可能是不现实的：除了对网络带宽造成负担外，向服务器发送本地数据违反了联合学习的关键隐私假设，并向所有设备发送全球共享代理数据需要努力仔细生成或收集此类辅助数据（额外代价）。</p>
</li>
<li><p>除了统计异构性之外，系统异构性也是联邦网络中的一个关键问题。由于硬件（CPU、内存）、网络连接（3G、4G、5G、wifi）和电源（电池电量）的变化，联邦网络中每个设备的存储、计算和通信能力可能会有所不同。这些系统级特性极大地加剧了诸如故障缓解和容错等挑战。一个实际中应用的策略是忽略那些无法完成一定数量训练的受限设备。然而（正如我们在第5节中所展示的），这可能会对收敛产生负面影响，因为它限制了有助于训练的有效设备的数量，并且如果丢弃的设备具有特定的数据特征，可能会导致采样过程中存在偏差。</p>
</li>
<li><p>在这项工作中，受FedAvg的启发，我们探索了一个更广泛的框架FedProx，它能够处理异构联邦环境，同时保持相似的隐私和计算优势。我们通过本地函数之间的统计数据的差异性来分析框架的收敛行为，同时考虑实际系统约束。我们的差异性表述受到求解线性方程组的随机化Kaczmarz方法的启发，类似的假设已用于分析其他环境下的SGD变体。我们提出的框架为异构联邦网络中的优化提供了改进的鲁棒性和稳定性。</p>
</li>
<li><p>最后，在相关工作方面，我们注意到，我们提出的工作的两个方面——FedProx中的近端项和我们分析中使用的有界差异性假设——已经在优化文献中进行了研究，尽管通常具有非常不同的动机和在非联邦环境中。为完整起见，我们在附录B中进一步讨论了这项背景工作。</p>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2></li>
</ul>
</li>
<li><p>FedAvg</p>
</li>
</ul>
<p>$$\min\limits_{w}f(w)=\sum_{k=1}^{N}p_kF_k(w)=\sum_{k=1}^{N}\frac{n_k}{n}F_k(w)=\mathbb{E}<em>{k}[F_k(w)],F_k(w)=\frac{1}{n_k}\sum</em>{i\in \mathcal{P}_k}f_i(w).$$<br>其中，$$F_k(w)$$可能为非凸函数。</p>
<pre><code>  - 为了降低通信成本，一个联邦优化中的一般技术是，对每个设备，应用一个基于设备数据的本地目标函数，用作全局目标函数的surrogate（替代？）。在每轮外部迭代中，一组设备的子集被选中且这些设备的本地解算器被用作优化本地目标函数。设备之后上传模型更新，服务器聚合并更新全局模型。
  - 联邦学习场景中，处理灵活性能的关键是每个本地目标函数可以进行非精确的解算。这意味着更多的本地迭代意味着更精确的本地结果。
  - $$\gamma$$-inexact solution
</code></pre>
<p>对函数$$h(w;w_0)=F(w)+\frac{\mu}{2}||w-w_0||^2$$，且$$\gamma\in[0,1]$$，我们说如果$$||\nabla h(w^*;w_0)||\le \gamma||\nabla h(w_0;w_0)||$$，其中$$\nabla h(w;w_0)=\nabla F(w)+\mu(w-w_0)$$，那么，$$w^*$$是$${\min}_wh(w;w_0)$$的一个$$\gamma$$-非精确解。<br>其中，越小的$$\gamma$$对应越高的精确度。</p>
<pre><code>  - 我们用$$\gamma$$-不确定性在Section 4的分析中，通过每轮（外部轮）的本地解算器衡量本地计算的amount（数量；量级；计算量）。在早期讨论中，不同的设备在解决本地子问题的时候由于不同的系统条件会执行不同的进程，因此变化的$$\gamma$$是很重要的，其随着设备的不同和轮次的不同变化。为了符号的简便，Section 4中将讨论相同的$$\gamma$$，之后在Corollary 9提供变化的$$\gamma$$。
  - FedAvg的参数设置
     - 设备k的本地代理：$$F_k(\cdot)$$
     - 本地解释器：SGD
     - 同学习率/同本地轮数E
        - drop：对固定时间未完成E轮本地更新的客户端进行drop操作。
     - 每轮，设备子集K&lt;&lt;N
  - McMahan论证了，FedAvg的超参优化是很重要的。尤其是，本地轮次的数量在收敛中起到了至关重要的影响。一方面，执行更多的本地论述允许更多的本地计算和潜在的通信减少，在通信受限的网络中可以大大提高总体的收敛速度（外部轮次降低）。另一方面，对于异构的本地目标函数$$F_k(\cdot)$$，大量的本地轮数可能导致每个设备的本地目标函数计算的最优参数与整体的目标函数相背——潜在的影响了收敛，更可能引起发散。而且，在异构系统联邦网络中，本地轮数过高可能提升设备给定通信轮数中无法完成训练的风险，而导致因此必须退出程序（指的是本地的训练参数与总体目标参数需求差距过大，导致在给定轮数无法收敛，影响最终性能，最终强制退出）（异构数据原因）。
  - 实际上，在稳健收敛的前提下，通过某种方式尽量提高本地学习的轮数是很重要，毕竟有助于降低通信轮数。更重要的是，我们注意到，本地轮数的最佳设置可能随着每次（外部？）迭代或者每个设备而改变--可看作为一个本地数据和可变系统资源的函数。事实上，一个比设定固定数量的本地轮数更自然的方法，是允许轮数根据网络特征变化和通过累计异质性谨慎地合并解决方案。我们在下文介绍的FedProx中正式确定了这一策略。
</code></pre>
<ul>
<li>FedProx<ul>
<li>与FedAvg类似<ul>
<li>每轮选择设备子集</li>
<li>执行本地更新，本地更新平均组成全局更新</li>
</ul>
</li>
<li>与FedAvg区别<ul>
<li>简单但关键的修改，可以实现重要的实验提升，也为我们的方法提供了收敛保证。</li>
</ul>
</li>
<li>特性1:容忍部分工作<ul>
<li>由于计算硬件、网络连接、电源等因素，设备通常具有不同等资源约束。每台设备相同的本地轮数不现实。</li>
<li>FedProx允许可变的本地工作量（基于系统资源），聚合部分解算（受限解算）。</li>
<li>根据不同的外部轮次t，和设备k，设定以下定义：</li>
<li>$$\gamma_k^t$$-inexact solution</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>对函数$$h_k(w;w_0)=F_k(w)+\frac{\mu}{2}||w-w_t||^2$$，且$$\gamma\in[0,1]$$，我们说如果$$||\nabla h_k(w^*;w_t)||\le \gamma_k^t||\nabla h_k(w_t;w_t)||$$，其中$$\nabla h_k(w;w_t)=\nabla F_k(w)+\mu(w-w_t)$$，那么，$$w^*$$是$${\min}_wh_k(w;w_t)$$的一个$$\gamma_k^t$$-非精确解。<br>因为显而易见的，$$\gamma_k^t$$越小，对应的本地精度越高，因此$$\gamma_k^t$$可以看作是某种计算程度/本地轮数/精确度的正相关关系。针对不同轮数，不同设备设定不同的$$\gamma_k^t$$，可以控制不同的本地目标函数停止的预期时间，实现对权重$$w^*$$的控制，也是实现对E的控制，尽管这部分工作可能是partial的，那么问题在于如何设定这样的$$\gamma_k^t$$呢？</p>
<pre><code>  - 特性2:近侧项（Appendix B）
     - 近侧项添加的意义：添加近侧项到本地子问题，来有效限制可变本地更新的影响。特别是，对比（取代）仅是最小化的本地损失函数，设备k使用选定的本地解算器来近似的最小化下面的目标函数：
</code></pre>
<p>$$\min_w h_k(w;w_0)=F_k(w)+\frac{\mu}{2}||w-w_t||^2$$<br>添加近侧项的意义：<strong>使得本地更新不要太过远离初始 global model w^t</strong>，在容忍系统异构性的前提下减少 Non-IID 的影响。</p>
<pre><code>     - 近侧项在两个方面将对算法有益：
        - 通过限制本地更新——使其更接近（是因为加入类似后避免了过拟合？）初始（全局）模型的方式解决了统计异构性的问题，而无需手动设置本地更新轮数。
        - 允许安全合并由系统异构性导致的可变数量的本地工作。我们总结了FedProx步骤如下：
</code></pre>
<p><img src="https://cdn.nlark.com/yuque/0/2021/png/22220737/1635391276449-66528b39-97ec-4d3c-a692-a686ae6c93a7.png#clientId=u67589db9-23d9-4&from=paste&height=400&id=u02b42f55&margin=%5Bobject%20Object%5D&name=image.png&originHeight=800&originWidth=1228&originalType=binary&ratio=1&size=175124&status=done&style=none&taskId=u77b545d7-928c-4db5-be4d-65bddcade26&width=614" alt="image.png"></p>
<pre><code>     - 联邦学习要解决的是更加一般性的问题，因此合理的假设是必要的（Section 4）：
        - non-IID数据，而非理想的IID数据
        - 本地解算器，而非一致的SGD
        - 多变的不完全的本地更新，而非完整
        - 每轮的设备子集，而非全部
     - Section 5的实验证明，在存在系统异构性的情况下，运行部分工作是有益的，并且与FedAvg相比，改进的局部子问题$$&#123;\min&#125;_wh_k(w;w_t)$$可以使异构数据集的收敛更加稳健和稳定。FedAvg是SGD计算固定轮数提交w，FedProx是计算目标函数，等待动态轮数后求得符合的w。在第4节中，我们还看到，使用近似项使FedProx更易于进行理论分析（即，本地目标函数可能表现得更好）。特别是，如果相应地选择μ，则Hessian函数可能是半正定的。因此，当$$F_k(\cdot)$$为非凸时，$$h_k$$为凸，当$$F_k(\cdot)$$为凸时，$$h_k$$为μ-强凸。
     - FedAvg是FedProx的一种特例：μ=0；解算器均为SGD；常数γ（对应于本地轮数），无关设备与外部轮数；Prox更加通用。
</code></pre>
<h2 id="FedProx的收敛分析"><a href="#FedProx的收敛分析" class="headerlink" title="FedProx的收敛分析"></a>FedProx的收敛分析</h2><ul>
<li>首先提出一套衡量标准具体衡量本地函数之间的差异性，之后在IID假设下分析FedProx，其中考虑了动态的γ。</li>
<li>本地差异性（local dissimilarity）<ul>
<li>我们引入一种联邦网络中设备之间的差异性度量方式，其足以证明收敛性。其也可以通过更简单和更严格的梯度有界方差假设（Corollary 10）来满足，我们在第5节的实验中对此进行了探索。有趣的是，类似的假设已在其他地方进行了探索，但目的不同；我们在附录中对这些工作进行了讨论。</li>
<li>$$B$$-本地差异性/区分度：</li>
</ul>
</li>
</ul>
<p>如果满足$$\mathbb{E}_k[||\nabla F_k(w)||^2]\le||\nabla f(w)||^2B^2$$，我们就称本地代价函数$$F_k(\cdot)$$在$$w$$处是$$B$$-本地区分的。我们进一步定义$$B(w)=\sqrt{\frac{\mathbb{E}<em>k[||\nabla F_k(w)||^2]}{||\nabla f(w)||^2}}$$，其中$$||\nabla f(w)||^2 \neq 0$$。<br>通过数学归纳法，可以看出该B(w)值，随着k每增加1，$$k\sum F_i^2-(\sum F_i)^2$$的差值就会增加k个$$F^2_i-F^2</em>{(k+1)},1 \le i \le k.$$所以相对应的比值也会增加，由此可理解为，k从0开始的每次加1，都是叠加新加入客户端与之前所有客户端的损失值差的平方。因此，B(w)会越来愈大。</p>
<pre><code>  - 基于$$B$$-本地差异性/区分度，我们提出正式的差异性假设，并将其用于收敛分析。这仅仅要求B(w)是有界的。入后所述，我们的收敛率是一个关于统计异质性和设备差异性的函数。
  - 假设 1:有界差异性
</code></pre>
<p>对部分$$\epsilon&gt;0$$，存在一个$$B_{\epsilon}$$，使得对所有的点$$w\in \mathcal{S}^c_{\epsilon}={w|\ ||\nabla f(w)||^2&gt;\epsilon},B(w)\le B_{\epsilon}$$。<br>这个假设的直观含义是，当全局模型没有收敛时(将梯度范数的平方小于ϵ视作收敛)，局部模型之间的差别是有界的。<br>简单理解一下，就是给定一个全局目标的量级的情况下（大于一个数），所有大于该量级的权重，它的本地区分度都小于此边界值。<br>为什么存在这个假设？这个假设符合一般规律吗？</p>
<pre><code>  - 大多数的学习问题并不需要高精度的稳定解，过小的$$\epsilon&gt;0$$并不能证明学习性能好，反而会因为过度拟合导致泛化性能差。可以合理假设，本地函数之间是存在相异性的。相异性越小，收敛越好。5.3.3中对真实数据集计算了相异性。
</code></pre>
<ul>
<li>FedProx分析<ul>
<li>定理4:非凸FedProx收敛：B-本地区分度</li>
</ul>
</li>
</ul>
<p>令假设1成立，假定本地目标函数$$F_k$$是非凸，L-Lipschitz平滑，存在$$L_{_}&gt;0$$，使$$\nabla^2F_k\succeq L_{_}I$$，其中$$\overline\mu:=\mu-L_{_}&gt;0.$$假设$$w^t$$并非一个稳定解且本地目标函数为B-差异的，即$$B(w^t)\le B.$$如果算法2中的参数$$\mu,K,\gamma$$已被选择，那么：($$\rho&gt;0$$)<br>$$\rho=(\frac{1}{\mu}-\frac{\gamma B}{\mu}-\frac{B(1+\gamma)\sqrt 2}{\overline{\mu}\sqrt K}-\frac{LB(1+\gamma)}{\overline\mu\mu}-\frac{L(1+\gamma)^2B^2}{2\overline\mu^2}-\frac{LB^2(1+\gamma^2)}{\overline\mu^2K}(2\sqrt {2K}+2))&gt;0$$<br>之后对于算法2中的每次迭代，我们有下列对全局目标的期望下降：<br>$$\mathbb{E}_{S_t}[f(w^{t+1})]\le f(w^t)-\rho||\nabla f(w^t)||^2$$<br>$$\rho$$的存在，确保了每个外部轮都有稳定的，可预计的下降，这是确保收敛性能的前提，也是收敛轮数O()的前提。</p>
<pre><code>  - Remark 5:参数控制
  - 定理6:收敛率：FedProx：给定条件，以及满足定理4的假设条件，给出FedProx的轮数O()。
  - 引理7:收敛--凸案例：令定理4假设成立，但修改Fk为凸函数，而非非凸函数。给定B和$$\gamma^t_k=0$$，可以计算出$$\mu,\rho$$。
  - Remark 8:和SGD的比较：FedProx收获和SGD相同的渐近的收敛保证。在有界方差假设，对小的$$\epsilon$$，如果置换$$B_&#123;\epsilon&#125;$$为其引理十中的上界，并选取足够大的$$\mu$$。迭代复杂度和SGD相同。
</code></pre>
<p>我们的FedProx分析并不能证明其比分布式SGD性能更好，甚至事实上FedProx会在每轮本地执行更多的工作。事实上，当数据通过非独立同分布生成时，本地的更新可能表现差于分布式SGD。但是，他们对FedProx的收敛提供了充分不必要条件。我们的分析是第一个意识到分析任何联邦优化方法for异构情况下的问题一的。</p>
<pre><code>  - 引理 9：变化的$$\gamma$$:$$\rho^t,\mathbb&#123;E&#125;_&#123;S_t&#125;[f(w^&#123;t+1&#125;)]\le f(w^t)-\rho^t||\nabla f(w^t)||^2$$。
</code></pre>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul>
<li>Section 5.2：展示了FedProx针对系统异构下的未完整求解提升性能。</li>
<li>Section 5.3:展示了Prox统计异构下的有效性（无论系统异构性）。<ul>
<li>Section 5.3.1：研究了统计异构性的收敛</li>
<li>Section 5.3.: 展示了实验收敛是如何关联我们的理论有界差异假设（Assumption 1）的。</li>
<li>github.com/litian96/FedProx</li>
</ul>
</li>
<li>合成数据</li>
<li>真实数据：1.凸分类问题 mnist 多项式逻辑回归；2. 62类分类 联邦扩展Mnist（FEMnist）同模型；3. 非凸设置，文本情感分析任务on推文 Sentiment140 LSTM分类器 每个推特账户对应于一个设备；4. 下一个字符预测 莎翁全集 每个说话的角色被关联成一个设备。</li>
<li>实现/部署：统一采样设备，然后使用与本地数据点数量成比例的权重平均更新</li>
<li>超参/评价指标：假设每个通信轮对应一个特定的聚合时间戳</li>
</ul>
<p>怎么设定的动态轮数？</p>
<ul>
<li><p>系统异质性模拟：设定本地轮数上限E，对于完成部分工作的clients（分别测试0，50，90%的stragglers）执行随机化的本地轮数x，x属于[1,E]。</p>
<ul>
<li>额外的两种less异构环境：<ul>
<li><ol>
<li>首先设定了所有的设备的E=1，以类似的方式添加系统异构性。</li>
</ol>
</li>
<li><ol start="2">
<li>without任何统计异构性，使用同分布的合成数据集。FedAvg在设备失败时更健壮。</li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li><p>收敛性能讲随着统计异构性提高而变差。</p>
<ul>
<li>​</li>
</ul>
</li>
<li><p>参数u：u=0时，reduce to FedAvg。</p>
</li>
<li><p>实验图1:u&gt;0时，对所有百分比的stragglers都具有更低的训练损失。</p>
</li>
<li><p>实验图2：对合成数据集的计算，系统异质性越高，越合理，损失越低。u的存在可能降低IID数据情况下的收敛性能，但可以显著提高非IID情况下的收敛性能。</p>
</li>
</ul>
<p>​</p>
<p>​</p>
<p>总结：本文总共做了两件事：提出了近侧项，完成了证明，近侧项相当于解除过拟合，实则是对过高本地的优化，使得本地可以在本地工作不同量级的情况下，完成收敛，经过证明，在一定假设前提下，确实可以做到每轮可预计的收敛，最终实现可预计的轮数量级。但存在的问题是，系统异构性是虚构的，数据是人造合成的，导致实验中轮数是随机生成的，和数据和系统本身并没有很好的关联（可以改进吗）。原本的gamma可以限定轮数，是有包含关系的，但是实验直接在给定轮数的前提下进行试验，实验的内容为每个客户端执行不同的轮数，最终的参数聚合观察和原有的FedAvg的效果。另一个实验是在相同异构性的系统条件（一样轮数）下，用合成数据（未使用数据集）执行non-IID数据的实验，发现Prox的收敛性能好于Avg。<br>​</p>
<p>在最后的实验中，系统异构性被等价于了跑的本地轮数。<br>​</p>
<p>实际上相当于用近侧项取消了过拟合的问题，让non-IID在加入近侧项的算法里得到更好的效果，同时算法基于强凸函数经过了数学验证。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/11/02/hello-world/" rel="prev" title="Hello World">
                  <i class="fa fa-chevron-left"></i> Hello World
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/11/02/test-blog/" rel="next" title="test_blog">
                  test_blog <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
